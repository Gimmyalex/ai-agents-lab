import Screenshot from "@site/src/components/Screenshot";

# üëê Define agent tools

The easiest way to define custom tools for agents in LangChain is using the `@tool` decorator. The decorator makes tools out of functions by using the function name as the tool name by default, and the function's docstring as the tool's description.

We want the AI research agent to have access to the following tools:

* `get_paper_metadata_arxiv`: Fetches a list of research papers from Arxiv, given a topic

* `get_paper_summary_from_arxiv`: Fetches the summary for a single research paper, given a paper ID

* `answer_questions_about_topics`: Answer questions about a given topic based on information in the agent's knowledge base

The `get_paper_metadata_arxiv` tool has been defined for you under "Step 5: Create Agent Tools". Use this as an example to create the other two tools. The tool names and docstrings have been written out for you. All you have to do is code up the tool logic.

## Tool to fetch a list of titles from Arxiv

üëê Run the cell under "Tool to fetch a list of titles from Arxiv".

## Tool to fetch the summary of a paper

Fill in `<CODE_BLOCK_8>` under "Tool to fetch the summary of a paper" and run the cell containing the code block.

üëê **CODE_BLOCK_8**: Create a tool that uses the `ArxivLoader` document loader in LangChain to return the summary of a research paper, given the paper ID (`id`).
:::tip
* Use the `get_summaries_as_docs` method of the `ArxivLoader` class
* Handle the case where the paper ID is invalid i.e. number of docs returned from `ArxivLoader` are `0`
:::

üìö https://python.langchain.com/v0.1/docs/integrations/document_loaders/arxiv/

üìö https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.arxiv.ArxivLoader.html

<details>
<summary>Answer</summary>
<div>
```python
doc = ArxivLoader(query=id, load_max_docs=1).get_summaries_as_docs()
if len(doc) == 0:
    return "No summary found for this paper."
return doc[0].page_content
```
</div>
</details>

## Tool to answer questions based on information in the knowledge base

Earlier in this section, we created a knowledge base for the agent. This tool should use the knowledge base to help the agent answer questions about topics. To do this, you will need to:

* Create a Vector Search Index

* Create a MongoDB Vector Store retriever

* Create a RAG chain that uses the retriever and LLM to answer questions

### üëê Create a Vector Search Index

To retrieve documents from the knowledge base using Vector Search, you must configure a vector search index on the knowledge base collection.

Open the **Database Deployments** page in the Atlas UI and select **Create Index** in the lower right corner under Atlas Search.

<Screenshot url="https://cloud.mongodb.com" src="img/screenshots/40-agent-tools/1-create-index.png" alt="Select create index" />

Click the **Create Search Index** button.

<Screenshot url="https://cloud.mongodb.com" src="img/screenshots/40-agent-tools/2-create-search-index.png" alt="Create search index" />

Click **JSON Editor** under Atlas Vector Search to create your index

<Screenshot url="https://cloud.mongodb.com" src="img/screenshots/40-agent-tools/3-json-editor.png" alt="The 'Create Index' page with the 'JSON Editor' tab highlighted" />


Select the `mongodb_agents_lab` database and the `knowledge` collection, change the index name to `vector_index`, and add the following index definition in the JSON editor:

```python
{
  "fields": [
    {
      "type": "vector",
      "path": "embedding",
      "numDimensions": 1024,
      "similarity": "cosine"
    }
  ]
}
```

:::info
The number of dimensions in the index definition is 1024 since the Arxiv dataset we used to create the knowledge base uses Mixedbread AI's open-source [mxbai-embed-large-v1](https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1) model for embeddings.
:::

### Create a MongoDB Vector Store Retriever

To retrieve documents from the knowledge base using vector search in LangChain, you must create a vector store object and create a retriever interface for it.

üëê Run the **FIRST** cell under "Tool to answer questions based on information in the knowledge base" that specifies the embedding model to use for vector search. We use the same model that we used for the knowledge base i.e. `mxbai-embed-large-v1`.

Fill in `<CODE_BLOCK_9>` and `<CODE_BLOCK_10>`, and run the cells containing these code blocks. 

üëê **CODE_BLOCK_9**: Create a MongoDBAtlas vector store object using the `from_connection_string` method of the `MongoDBAtlasVectorSearch` class in the `langchain-mongodb` integration

:::tip
* Be sure to pass these arguments to the `from_connection_string` method: `connection_string`, `namespace`, `embedding`, `index_name`, `text_key`
* Use the variables defined under "Step 3: Ingest Data into MongoDB Atlas"
:::

üìö https://api.python.langchain.com/en/latest/_modules/langchain_mongodb/vectorstores.html#MongoDBAtlasVectorSearch

<details>
<summary>Answer</summary>
<div>
```python
MongoDBAtlasVectorSearch.from_connection_string(
    connection_string=MONGODB_URI,
    namespace=DB_NAME + "." + COLLECTION_NAME,
    embedding=embedding_model,
    index_name=ATLAS_VECTOR_SEARCH_INDEX_NAME,
    text_key="abstract",
)
```
</div>
</details>

üëê **CODE_BLOCK_10**: Construct a retriever interface for the vector store, using `similarity` as the search type and `k` as `5` to retrieve the top 5 documents from the knowledge base

üìö https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/vectorstore/

<details>
<summary>Answer</summary>
<div>
```python
vector_store.as_retriever(search_type="similarity", search_kwargs={"k": 5})
```
</div>
</details>

### Create a RAG chain

Finally, create a RAG chain using LangChain's LCEL notation as the main logic of the `answer_questions_about_topics` tool.

Fill in `<CODE_BLOCK_11>` and run the cell containing the code block.

üëê **CODE_BLOCK_11**: Create a RAG chain using LCEL

:::tip
* Use the variables defined previously, such as `llm`, `retriever`, while creating the chain
* Return the response of running `invoke` on the chain with `query` as an argument
:::

üìö https://python.langchain.com/v0.1/docs/use_cases/question_answering/quickstart/

<details>
<summary>Answer</summary>
<div>
```python
retrieve = {
    "context": retriever
    | (lambda docs: "\n\n".join([d.page_content for d in docs])),
    "question": RunnablePassthrough(),
}
# Defining the chat prompt
template = """Answer the question based only on the following context. If no context is provided, say I do not know: \
{context}

Question: {question}
"""
prompt = ChatPromptTemplate.from_template(template)
# Parse output as a string
parse_output = StrOutputParser()
# Retrieval chain
retrieval_chain = retrieve | prompt | llm | parse_output

answer = retrieval_chain.invoke(query)

return answer
```
</div>
</details>

## Create a list of tools

üëê Run the cell just **BEFORE** "Test out the tools"